{
  "datasets": [
    {
      "name": "TrustAIRLab/in-the-wild-jailbreak-prompts",
      "description": "In-the-wild jailbreak prompts collected from various sources.",
      "config": "jailbreak_2023_05_07",
      "harmful_field": "prompt",
      "harmful_condition": "jailbreak == true"
    },
    {
      "name": "deepset/prompt-injections",
      "description": "Dataset for prompt injection detection.",
      "harmful_field": "text",
      "harmful_condition": "label == 1"
    },
    {
      "name": "jackhhao/jailbreak-classification",
      "description": "Jailbreak prompts for classification.",
      "harmful_field": "prompt",
      "harmful_condition": "type == 'jailbreak'"
    },
    {
      "name": "qualifire/prompt-injections-benchmark",
      "description": "Benchmark dataset for prompt injection vs benign.",
      "harmful_field": "text",
      "harmful_condition": "label == 'jailbreak'"
    },
    {
      "name": "jayavibhav/prompt-injection-safety",
      "description": "Prompt injection safety dataset.",
      "harmful_field": "text",
      "harmful_condition": "label == 1"
    },
    {
      "name": "dvilasuero/jailbreak-classification-reasoning",
      "description": "Jailbreak classification with reasoning.",
      "harmful_field": "prompt",
      "harmful_condition": "type == 'jailbreak'"
    },
    {
      "name": "Alignment-Lab-AI/Prompt-Injection-Test",
      "description": "Prompt injection test dataset.",
      "harmful_field": "red",
      "harmful_condition": "always harmful"
    },
    {
      "name": "Simsonsun/JailbreakPrompts",
      "description": "Collection of jailbreak prompts.",
      "config": "default",
      "harmful_field": "Prompt",
      "harmful_condition": "all prompts are harmful"
    },
    {
      "name": "JailbreakBench/JBB-Behaviors",
      "description": "Jailbreak behaviors dataset.",
      "config": "behaviors",
      "harmful_field": "Goal",
      "harmful_condition": "all goals are harmful"
    },
    {
      "name": "allenai/tulu-3-do-anything-now-eval",
      "description": "Do Anything Now (DAN) jailbreak evaluation dataset containing harmful prompts.",
      "config": "default",
      "harmful_field": "prompt",
      "harmful_condition": "always harmful"
    },
    {
      "name": "walledai/JailbreakHub",
      "description": "In-the-wild jailbreak prompts collected from various sources, including 1,405 identified jailbreak prompts.",
      "harmful_field": "prompt",
      "harmful_condition": "jailbreak == true"
    },
    {
      "name": "JailbreakV-28K/JailBreakV-28k",
      "description": "JailBreakV-28K dataset containing 28,000 jailbreak prompts with harmful queries.",
      "config": "JailBreakV_28K",
      "split": "JailBreakV_28K",
      "harmful_field": "jailbreak_query",
      "harmful_condition": "always harmful"
    },
    {
      "name": "collinear-ai/jailbreak-prompts-allam-allam-2-34b-dev-responses",
      "description": "Jailbreak prompts and responses from Allam-2-34b model.",
      "config": "default",
      "harmful_field": "prompt",
      "harmful_condition": "always harmful"
    },
    {
      "name": "walledai/MaliciousInstruct",
      "description": "Malicious instruction prompts designed to elicit harmful responses from language models.",
      "harmful_field": "prompt",
      "harmful_condition": "always harmful"
    },
    {
      "name": "llm-semantic-router/jailbreak-detection-dataset",
      "description": "Jailbreak detection dataset containing prompts labeled as jailbreak attempts.",
      "harmful_field": "text",
      "harmful_condition": "label_text == 'jailbreak'"
    },
    {
      "name": "walledai/HarmBench",
      "description": "HarmBench dataset containing harmful prompts across multiple categories including contextual attacks, copyright violations, and standard jailbreaks.",
      "config": ["contextual", "copyright", "standard"],
      "harmful_field": "prompt",
      "harmful_condition": "always harmful"
    },
    {
      "name": "walledai/AdvBench",
      "description": "AdvBench dataset containing adversarial prompts designed to jailbreak language models.",
      "harmful_field": "prompt",
      "harmful_condition": "always harmful"
    },
    {
      "name": "allenai/wildguardmix",
      "description": "WildGuardMix dataset containing mixed safety and harmful prompts for training and testing safety classifiers.",
      "config": ["wildguardtest", "wildguardtrain"],
      "harmful_field": "prompt",
      "harmful_condition": "prompt_harm_label == 'harmful'"
    },
    {
      "name": "wow2000/multilingual_jailbreak_challenges",
      "description": "Multilingual jailbreak challenges dataset containing adversarial prompts in multiple languages.",
      "harmful_field": "en-user_instruction",
      "harmful_condition": "always harmful"
    }
  ],
  "github": [
    {
      "name": "0xk1h0/ChatGPT_DAN",
      "description": "ChatGPT DAN jailbreak prompts",
      "url": "https://github.com/0xk1h0/ChatGPT_DAN",
      "harmful_field": "prompt",
      "harmful_condition": "always harmful",
      "type": "single-level",
      "file_path": "README.md",
      "format": "text"
    },
    {
      "name": "coolaj86/6f4f7b30129b0251f61fa7baaa881516",
      "description": "ChatGPT DAN jailbreak prompts",
      "url": "https://gist.github.com/coolaj86/6f4f7b30129b0251f61fa7baaa881516",
      "harmful_field": "prompt",
      "harmful_condition": "always harmful",
      "type": "single-level",
      "file_path": "*.md",
      "format": "text"
    },
    {
      "name": "RamamAgarwal/90dd19cfee89fb260c4461e57b8f7a7a",
      "description": "ChatGPT DAN jailbreak prompts",
      "url": "https://gist.github.com/RamamAgarwal/90dd19cfee89fb260c4461e57b8f7a7a",
      "harmful_field": "prompt",
      "harmful_condition": "always harmful",
      "type": "single-level",
      "file_path": "*.md",
      "format": "text"
    },
    {
      "name": "langgptai/awesome-grok-prompts",
      "description": "Awesome Grok prompts including jailbreaks",
      "url": "https://github.com/langgptai/awesome-grok-prompts",
      "harmful_field": "prompt",
      "harmful_condition": "always harmful",
      "type": "single-level",
      "file_path": "README.md",
      "format": "text"
    },
    {
      "name": "PromptLabs/Prompt-Hacking-Resources",
      "description": "Prompt hacking resources including jailbreaks",
      "url": "https://github.com/PromptLabs/Prompt-Hacking-Resources",
      "harmful_field": "prompt",
      "harmful_condition": "always harmful",
      "type": "single-level",
      "file_path": "README.md",
      "format": "text"
    },
    {
      "name": "liudaizong/Awesome-LVLM-Attack",
      "description": "Awesome list of LVLM attacks including jailbreaks",
      "url": "https://github.com/liudaizong/Awesome-LVLM-Attack",
      "harmful_field": "prompt",
      "harmful_condition": "always harmful",
      "type": "single-level",
      "file_path": "README.md",
      "format": "text"
    },
    {
      "name": "EasyJailbreak/EasyJailbreak",
      "description": "EasyJailbreak framework for jailbreaking LLMs",
      "url": "https://github.com/EasyJailbreak/EasyJailbreak",
      "harmful_field": "prompt",
      "harmful_condition": "always harmful",
      "type": "single-level",
      "file_path": "README.md",
      "format": "text"
    },
    {
      "name": "IntelLabs/LLMart",
      "description": "LLM adversarial robustness toolkit",
      "url": "https://github.com/IntelLabs/LLMart",
      "harmful_field": "prompt",
      "harmful_condition": "always harmful",
      "type": "single-level",
      "file_path": "README.md",
      "format": "text"
    },
    {
      "name": "0x6f677548/copilot-instructions-unicode-injection",
      "description": "Copilot instructions unicode injection PoC",
      "url": "https://github.com/0x6f677548/copilot-instructions-unicode-injection",
      "harmful_field": "prompt",
      "harmful_condition": "always harmful",
      "type": "single-level",
      "file_path": "README.md",
      "format": "text"
    },
    {
      "name": "SheltonLiu-N/AutoDAN",
      "description": "ADVBench dataset containing harmful behavior prompts for evaluating jailbreak attacks on LLMs",
      "url": "https://github.com/SheltonLiu-N/AutoDAN",
      "harmful_field": "goal",
      "harmful_condition": "always harmful",
      "type": "single-level",
      "file_path": "data/advbench/harmful_behaviors.csv",
      "format": "csv"
    },
    {
      "name": "AmanPriyanshu/FRACTURED-SORRY-Bench-Automated-Multishot-Jailbreaking",
      "description": "FRACTURED-SORRY-Bench dataset containing multi-turn jailbreak prompts decomposed into sequential steps",
      "url": "https://github.com/AmanPriyanshu/FRACTURED-SORRY-Bench-Automated-Multishot-Jailbreaking",
      "harmful_field": "decomposed_steps",
      "harmful_condition": "always harmful",
      "type": "multi-level",
      "file_path": "data/question_breakdowns.jsonl",
      "format": "jsonl"
    }
  ]
}
