@inproceedings{shen2024_inwild_jailbreaks,
  title        = {{``Do Anything Now'': Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models}},
  author       = {Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
  booktitle    = {Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security (CCS)},
  year         = {2024},
  url          = {https://arxiv.org/abs/2308.03825},
  note         = {CCS 2024 version; preprint available on arXiv.}
}

@misc{deepset_prompt_injections_hf,
  title        = {deepset/prompt-injections},
  author       = {deepset},
  year         = {2024},
  howpublished = {\url{https://huggingface.co/datasets/deepset/prompt-injections}},
  note         = {Hugging Face dataset; accessed 2025-10-29}
}

@misc{jackhhao_jailbreak_classification_hf,
  title        = {jackhhao/jailbreak-classification},
  author       = {Hao, Jack},
  year         = {2023},
  howpublished = {\url{https://huggingface.co/datasets/jackhhao/jailbreak-classification}},
  note         = {Hugging Face dataset; accessed 2025-10-29}
}

@misc{qualifire_prompt_injections_benchmark_hf,
  title        = {qualifire/prompt-injections-benchmark},
  author       = {Qualifire AI},
  year         = {2025},
  howpublished = {\url{https://huggingface.co/datasets/qualifire/prompt-injections-benchmark}},
  note         = {Hugging Face dataset; accessed 2025-10-29}
}

@misc{jayavibhav_prompt_injection_safety_hf,
  title        = {jayavibhav/prompt-injection-safety},
  author       = {Vibhav, Jaya},
  year         = {2024},
  howpublished = {\url{https://huggingface.co/datasets/jayavibhav/prompt-injection-safety}},
  note         = {Hugging Face dataset; accessed 2025-10-29}
}

@misc{dvilasuero_jailbreak_classification_reasoning_hf,
  title        = {dvilasuero/jailbreak-classification-reasoning},
  author       = {Vilasuero, D.},
  year         = {2024},
  howpublished = {\url{https://huggingface.co/datasets/dvilasuero/jailbreak-classification-reasoning}},
  note         = {Hugging Face dataset; accessed 2025-10-29}
}

@misc{alignmentlab_prompt_injection_test_hf,
  title        = {Alignment-Lab-AI/Prompt-Injection-Test},
  author       = {Alignment Lab AI},
  year         = {2024},
  howpublished = {\url{https://huggingface.co/datasets/Alignment-Lab-AI/Prompt-Injection-Test}},
  note         = {Hugging Face dataset; accessed 2025-10-29}
}

@misc{simsonsun_jailbreakprompts_hf,
  title        = {Simsonsun/JailbreakPrompts},
  author       = {Knuts, Simon},
  year         = {2025},
  howpublished = {\url{https://huggingface.co/datasets/Simsonsun/JailbreakPrompts}},
  note         = {Hugging Face dataset; accessed 2025-10-29}
}

@misc{guychuk_open_prompt_injection_hf,
  title        = {guychuk/open-prompt-injection},
  author       = {Nachshon, Guy},
  year         = {2025},
  howpublished = {\url{https://huggingface.co/datasets/guychuk/open-prompt-injection}},
  note         = {Hugging Face dataset; accessed 2025-10-29}
}

@inproceedings{chao2024_jailbreakbench,
  title        = {JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models},
  author       = {Chao, Pin-Jie and Xu, Xin-Chao and Zhang, Zhengyan and Zhang, Yipeng and Zhou, Kun and Tan, Zhixing and Xu, Han and Liu, Daogao and An, Xinyu and Hao, Shibo and Wang, Yiming and Mathew, Binny and Hauer, Bradley and Jurgens, David and Gupta, Manish and Zhu, Wenhao and Shen, Shiwei and Guo, Zhen and Li, Zong-An and Yin, Bing and Qiu, Xipeng and Sun, Xu},
  booktitle    = {NeurIPS 2024 Datasets and Benchmarks Track},
  year         = {2024},
  url          = {https://openreview.net/forum?id=q3PpXmSTO0}
}

@misc{allenai_tulu3_dan_eval_hf,
  title        = {allenai/tulu-3-do-anything-now-eval},
  author       = {Allen Institute for AI},
  year         = {2025},
  howpublished = {\url{https://huggingface.co/datasets/allenai/tulu-3-do-anything-now-eval}},
  note         = {Hugging Face dataset; accessed 2025-10-29}
}

@inproceedings{shen2024_jailbreakhub,
  title        = {{``Do Anything Now'': Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models}},
  author       = {Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
  booktitle    = {Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security (CCS)},
  year         = {2024},
  url          = {https://arxiv.org/abs/2308.03825},
  note         = {Cites JailbreakHub framework used to curate 1{,}405 in-the-wild jailbreak prompts.}
}

@misc{jailbreakv28k_2024_arxiv,
  title        = {JailBreakV-28K: A Benchmark for Assessing the Robustness of Multimodal Large Language Models against Jailbreak Attacks},
  author       = {Luo, Weidi and Ma, Siyuan and Liu, Xiaogeng and Guo, Xiaoyu and Xiao, Chaowei},
  year         = {2024},
  eprint       = {2404.03027},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  howpublished = {\url{https://arxiv.org/abs/2404.03027}},
  note         = {Preprint; associated dataset: \url{https://huggingface.co/datasets/JailbreakV-28K/JailBreakV-28k}}
}

@misc{rubend18_chatgpt_jailbreak_prompts_hf,
  title        = {rubend18/ChatGPT-Jailbreak-Prompts},
  author       = {rubend18},
  year         = {2023},
  howpublished = {\url{https://huggingface.co/datasets/rubend18/ChatGPT-Jailbreak-Prompts}},
  note         = {Hugging Face dataset; accessed 2025-10-29}
}

@misc{collinear_ai_allam34b_jailbreak_prompts_hf,
  title        = {collinear-ai/jailbreak-prompts-allam-allam-2-34b-dev-responses},
  author       = {Collinear AI},
  year         = {2025},
  howpublished = {\url{https://huggingface.co/datasets/collinear-ai/jailbreak-prompts-allam-allam-2-34b-dev-responses}},
  note         = {Hugging Face dataset; accessed 2025-10-29}
}

@misc{guychuk_open_prompt_injection_hf_dup,
  title        = {guychuk/open-prompt-injection},
  author       = {Nachshon, Guy},
  year         = {2025},
  howpublished = {\url{https://huggingface.co/datasets/guychuk/open-prompt-injection}},
  note         = {Hugging Face dataset; accessed 2025-10-29; duplicate safeguard key omitted if already included.}
}


@inproceedings{
      liu2024autodan,
      title={AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models},
      author={Xiaogeng Liu and Nan Xu and Muhao Chen and Chaowei Xiao},
      booktitle={The Twelfth International Conference on Learning Representations},
      year={2024},
      url={https://openreview.net/forum?id=7Jwpw4qKkb}
}

@misc{priyanshu2024fracturedsorrybench,
  title        = {FRACTURED-SORRY-Bench},
  author       = {Aman Priyanshu and Supriti Vijay},
  year         = {2024},
  howpublished = {\url{https://github.com/AmanPriyanshu/FRACTURED-SORRY-Bench}},
  note         = {Licensed under CC BY-NC 4.0}
}

@article{gandalf_paper,
  title={Gandalf the Red: Adaptive Security for LLMs},
  author={Pfister, Niklas and Volhejn, Václav and Knott, Manuel and Arias, Santiago and Bazínska, Julia and Bichurin, Mykhailo and Commike, Alan and Darling, Janet and Dienes, Peter and Fiedler, Matthew and others},
  journal={arXiv preprint arXiv:2501.07927},
  year={2025}
}

@article{yi2023benchmarking,
  title={Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models},
  author={Yi, Jingwei and Xie, Yueqi and Zhu, Bin and Hines, Keegan and Kiciman, Emre and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao},
  journal={arXiv preprint arXiv:2312.14197},
  year={2023}
}
